{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "16b4084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957ff4a",
   "metadata": {},
   "source": [
    "# Gender inference\n",
    "\n",
    "Many of the conversations in the manosphere focus on relationships between men and women. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "17059677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/mnt/data0/lucy/manosphere/'\n",
    "ANN_FILE = ROOT + 'data/ann_sig_entities.csv'\n",
    "COREF_RESULTS = ROOT + 'logs/coref_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e15cadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary(): \n",
    "    words = []\n",
    "    with open(ANN_FILE, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader: \n",
    "            if row['keep'] == 'Y': \n",
    "                if row['entity'].lower() == 'she' or row['entity'].lower() == 'he': \n",
    "                    continue\n",
    "                words.append(row['entity'].lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34566b",
   "metadata": {},
   "source": [
    "Check that categories are all neat and clean with no NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "8e444fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Femcels', 'the_attraction', 'TRP', 'pua_forum', 'MRA', 'red_pill_talk', 'Incels', 'MGTOW', 'rooshv', 'mgtow', 'avfm', 'incels', 'FDS', 'PUA', 'CONTROL'}\n"
     ]
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(COREF_RESULTS + 'coref_reddit_df.csv')\n",
    "cats = set(reddit_df.community.unique()) | set(forum_df.community.unique())\n",
    "assert len(cats) == len(set(reddit_df.community.unique())) + len(set(forum_df.community.unique()))\n",
    "forum_df = pd.read_csv(COREF_RESULTS + 'coref_forum_df.csv')\n",
    "df = pd.concat([reddit_df, forum_df])\n",
    "control_df = pd.read_csv(COREF_RESULTS + 'coref_CONTROL_df.csv')\n",
    "all_df = pd.concat([reddit_df, forum_df, control_df])\n",
    "cats = set(all_df.community.unique())\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "7ef03b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% fem: 0.5852356166540743\n",
      "% masc: 0.4147643833459257\n"
     ]
    }
   ],
   "source": [
    "fem_total = sum(df['fem'].to_list())\n",
    "masc_total = sum(df['masc'].to_list())\n",
    "print(\"% fem:\", fem_total / (fem_total + masc_total))\n",
    "print(\"% masc:\", masc_total / (fem_total + masc_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c640f4",
   "metadata": {},
   "source": [
    "### Word coverage\n",
    "\n",
    "We want to get a sense of how many words are gendered. Words that are not gendered likely don't show up often enough to matter much, but there is a long tail that could be important to consider. \n",
    "\n",
    "First, we get a sense of how many words actually have coref labels (e.g. more than 20 labels): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e3454d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_coverage(this_df, cutoff, marked_set): \n",
    "    totals = this_df.groupby('word').sum()\n",
    "    totals['total'] = totals['fem'] + totals['masc']\n",
    "    totals = totals[totals['total'] > cutoff]\n",
    "    totals.sort_values(by=['total'])\n",
    "    solid_labels = totals.index.to_list()\n",
    "    vocab = load_vocabulary()\n",
    "    missing = set(vocab) - set(solid_labels) - marked_set\n",
    "    print(\"NO GENDER SIGNAL:\", len(missing) / len(vocab))\n",
    "    print(\"COREF SIGNAL:\",len(solid_labels)/ len(vocab))\n",
    "    print(\"MARKED SIGNAL:\", len(marked_set) / len(vocab))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "960ebecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO GENDER SIGNAL: 0.688549387384662\n",
      "COREF SIGNAL: 0.3114506126153381\n",
      "MARKED SIGNAL: 0.0\n",
      "\n",
      "NO GENDER SIGNAL: 0.8534261080018152\n",
      "COREF SIGNAL: 0.14657389199818485\n",
      "MARKED SIGNAL: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_dataframe_coverage(df, 20, set())\n",
    "get_dataframe_coverage(control_df, 20, set())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79c37f",
   "metadata": {},
   "source": [
    "The number of words without coref signals is pretty low! It seems like we need **_multiple ways_** to infer the gender of an entity. \n",
    "\n",
    "**Step 1**: semantically gendered nouns, or nouns gendered by definition\n",
    "\n",
    "We left out nouns that are socially gendered, e.g. \"nurse\". We use singular and plural forms. \n",
    "\n",
    "Hoyle et al. (2019) - man, men, boy, boys, father, fathers, son, sons, brother, bothers, husband, husbands, uncle, uncles, nephew, nephews, emperor, emperors, king, kings, prince, princes, duke, dukes, lord, lords, knight, knights, waiter, waiters, actor, actors, god, gods, policeman, policemen, postman, postmen, hero, heros, wizard, wizards, steward, stewards, woman, women, girl, girls, mother, mothers, daughter, daughters, sister, sisters, wife, wives, aunt, aunts, niece, nieces, empress, empresses, queen, queens, princess, princesses, duchess, duchesses, lady, ladies, dame, dames, waitress, waitresses, actress, actresses, goddess, goddesses, policewoman, policewomen, postwoman, postwomen, heroine, heroines, witch, witches, stewardess, stewardesses\n",
    "\n",
    "Additional - male, males, dude, dudes, guy, guys, boyfriend, boyfriends, bf, female, females, chick, chicks, girlfriend, girlfriends, gf, gal, gals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "00c2e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fathers', 'emperors', 'actors', 'boy', 'kings', 'dude', 'dudes', 'postmen', 'nephew', 'hero', 'male', 'steward', 'sons', 'postman', 'lord', 'bf', 'king', 'waiter', 'boyfriend', 'bothers', 'knights', 'wizards', 'brother', 'boyfriends', 'males', 'waiters', 'princes', 'gods', 'prince', 'uncle', 'guys', 'emperor', 'dukes', 'actor', 'nephews', 'boys', 'duke', 'father', 'husbands', 'knight', 'heros', 'policemen', 'wizard', 'stewards', 'men', 'son', 'husband', 'god', 'policeman', 'man', 'uncles', 'lords', 'guy'}\n",
      "{'girls', 'wives', 'chicks', 'empresses', 'princess', 'lady', 'queen', 'actresses', 'women', 'witches', 'princesses', 'gf', 'dame', 'stewardess', 'females', 'wife', 'actress', 'duchess', 'gal', 'heroines', 'nieces', 'girlfriend', 'ladies', 'aunts', 'waitress', 'girl', 'queens', 'policewoman', 'duchesses', 'policewomen', 'waitresses', 'goddesses', 'female', 'daughter', 'mothers', 'postwomen', 'aunt', 'heroine', 'gals', 'woman', 'sisters', 'daugheters', 'empress', 'chick', 'goddess', 'stewardesses', 'girlfriends', 'sister', 'dames', 'mother', 'niece', 'postwoman', 'witch'}\n"
     ]
    }
   ],
   "source": [
    "men_markers = 'man, men, boy, boys, father, fathers, son, sons, brother, bothers, husband, husbands, uncle, uncles, nephew, nephews, emperor, emperors, king, kings, prince, princes, duke, dukes, lord, lords, knight, knights, waiter, waiters, actor, actors, god, gods, policeman, policemen, postman, postmen, hero, heros, wizard, wizards, steward, stewards, '\n",
    "men_markers += 'male, males, dude, dudes, boyfriend, boyfriends, bf, guy, guys'\n",
    "men_markers = set(men_markers.split(', '))\n",
    "women_markers = 'woman, women, girl, girls, mother, mothers, daughter, daugheters, sister, sisters, wife, wives, aunt, aunts, niece, nieces, empress, empresses, queen, queens, princess, princesses, duchess, duchesses, lady, ladies, dame, dames, waitress, waitresses, actress, actresses, goddess, goddesses, policewoman, policewomen, postwoman, postwomen, heroine, heroines, witch, witches, stewardess, stewardesses, '\n",
    "women_markers += 'female, females, chick, chicks, girlfriend, girlfriends, gf, gal, gals'\n",
    "women_markers = set(women_markers.split(', '))\n",
    "print(men_markers)\n",
    "print(women_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "f459af47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count, fraction of vocab, examples\n",
      "MARKED MEN: 956 0.145 ['naked man', 'western men', 'best man', 'white dudes', 'brown men', 'confident guy', 'naked men', 'average men', 'boring guy', 'one boy']\n",
      "MARKED WOMEN: 1011 0.153 ['other women', 'female managers', 'married girl', 'uglier girls', 'we women', 'two girls', 'you ladies', 'hotter girls', 'slutty girls', 'second wife']\n"
     ]
    }
   ],
   "source": [
    "marked_vocab_men = set()\n",
    "marked_vocab_women = set()\n",
    "vocab = load_vocabulary()\n",
    "for w in vocab: \n",
    "    w_tokens = set(w.split())\n",
    "    if w_tokens & men_markers: \n",
    "        marked_vocab_men.add(w)\n",
    "    elif w_tokens & women_markers:\n",
    "        marked_vocab_women.add(w)\n",
    "marked_vocab = marked_vocab_men | marked_vocab_women\n",
    "print(\"Count, fraction of vocab, examples\")\n",
    "print(\"marked men:\".upper(), len(marked_vocab_men), round(len(marked_vocab_men) / len(vocab), 3), \n",
    "      random.sample(marked_vocab_men, 10))\n",
    "print(\"marked women:\".upper(), len(marked_vocab_women), round(len(marked_vocab_women) / len(vocab), 3), \n",
    "      random.sample(marked_vocab_women, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856b593",
   "metadata": {},
   "source": [
    "**Step 2:** coreference resolution for singular nouns \n",
    "\n",
    "First, we filter the dataframes to words that are not explicitly marked, and we again calculate coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f83b7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmarked_df = df[~df['word'].isin(marked_vocab)]\n",
    "unmarked_control_df = control_df[~control_df['word'].isin(marked_vocab)]\n",
    "unmarked_all_df = pd.concat([unmarked_df, unmarked_control_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0651ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO GENDER SIGNAL: 0.5162607774920587\n",
      "COREF SIGNAL: 0.18620481016487672\n",
      "MARKED SIGNAL: 0.2975344123430646\n",
      "\n",
      "NO GENDER SIGNAL: 0.5991529269399486\n",
      "COREF SIGNAL: 0.10331266071698685\n",
      "MARKED SIGNAL: 0.2975344123430646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_dataframe_coverage(unmarked_df, 20, marked_vocab)\n",
    "get_dataframe_coverage(unmarked_control_df, 20, marked_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b29cd0",
   "metadata": {},
   "source": [
    "**Step 3**: plural nouns take on gender of singular nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea37a82",
   "metadata": {},
   "source": [
    "**Step 4:** bigrams take on unigram gender if modifier does not change semantic gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782e665",
   "metadata": {},
   "source": [
    "### Popular fem words in manosphere++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "9c3ead5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_fem(cat, this_df): \n",
    "    cat_df = this_df[this_df.community == cat]\n",
    "    cat_totals = cat_df.groupby('word').sum()\n",
    "    cat_totals['total'] = cat_totals['fem'] + cat_totals['masc'] \n",
    "    # filter to only those that appear at least 10 times as she or he\n",
    "    cat_totals = cat_totals[cat_totals['total'] > 10] \n",
    "    cat_totals['fem_frac'] = cat_totals['fem'] / (cat_totals['fem'] + cat_totals['masc'])\n",
    "    cat_fem = cat_totals[cat_totals['fem_frac'] == 1]\n",
    "    return cat_fem.sort_values(by=['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6d7630c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Femcels-------\n",
      "Empty DataFrame\n",
      "Columns: [year, fem, masc, they, it, you, total, fem_frac]\n",
      "Index: []\n",
      "------the_attraction-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "duff         6021   11     0     0   1    0     11       1.0\n",
      "escort      16083   11     0     1   2    0     11       1.0\n",
      "playmate    14070   12     0     0   0    0     12       1.0\n",
      "hot blonde  12064   12     0     0   1    0     12       1.0\n",
      "stacy       10040   12     0     0   0    1     12       1.0\n",
      "------TRP-------\n",
      "                year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                           \n",
      "hot blonde     10080   11     0     0   0    0     11       1.0\n",
      "old bitch      14112   11     0     0   2    0     11       1.0\n",
      "potential ltr  12099   12     0     6   8    0     12       1.0\n",
      "milf           12099   12     0     1   9    0     12       1.0\n",
      "housekeeper    12093   12     0     0   0    0     12       1.0\n",
      "------pua_forum-------\n",
      "                year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                            \n",
      "cute brunette  12076   11     0     0    1    0     11       1.0\n",
      "granny         12067   11     0     0    0    0     11       1.0\n",
      "momma          12060   11     0     0    1    0     11       1.0\n",
      "exgf           16090   12     0     1    5    0     12       1.0\n",
      "pua community  24150   12     0    25  125    0     12       1.0\n",
      "------MRA-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "baby momma  16124   18     0     0   1    0     18       1.0\n",
      "porn star   18135   18     0     1   2    0     18       1.0\n",
      "momma       20145   34     0     0   1    1     34       1.0\n",
      "becky       14107   38     0     0   0    1     38       1.0\n",
      "------red_pill_talk-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "mommy        6045   11     0     0   0    0     11       1.0\n",
      "single mom   8062   24     0     0   3    0     24       1.0\n",
      "escort      10080   68     0     3  20    0     68       1.0\n",
      "------Incels-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "exwife      10085   12     0     0   2    0     12       1.0\n",
      "sloot        6050   13     0     0   0    0     13       1.0\n",
      "ugly bitch   8070   15     0     0   1    0     15       1.0\n",
      "daughters   12095   19     0    78   0    2     19       1.0\n",
      "hostess     18135   19     0     0   1    0     19       1.0\n",
      "------MGTOW-------\n",
      "                  year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                             \n",
      "dancer            8070   11     0     0   0    0     11       1.0\n",
      "sisterhood        8070   11     0     9   8    0     11       1.0\n",
      "feminist friend  10085   11     0     0   0    0     11       1.0\n",
      "housekeeper       8070   11     0     0   0    0     11       1.0\n",
      "ugly bitch       10085   13     0     0   1    0     13       1.0\n",
      "------rooshv-------\n",
      "                year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                           \n",
      "cute brunette  10070   11     0     0   0    0     11       1.0\n",
      "sis            12081   11     0     0   0    0     11       1.0\n",
      "one bitch      16116   14     0     0   1    0     14       1.0\n",
      "stepdaughter   12096   14     0     0   0    0     14       1.0\n",
      "milf           20145   15     0     5   5    0     15       1.0\n",
      "------mgtow-------\n",
      "              year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                         \n",
      "becky         8065   11     0     0   0    0     11       1.0\n",
      "hostess       8066   11     0     0   0    0     11       1.0\n",
      "housewife     8066   11     0     0   1    0     11       1.0\n",
      "escort        8062   14     0     2   8    0     14       1.0\n",
      "receptionist  8066   14     0     0   1    0     14       1.0\n",
      "------avfm-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "children    16124   12     0  1308   3    3     12       1.0\n",
      "whore       12087   12     0     0   3    0     12       1.0\n",
      "single mom  12087   16     0     0   3    1     16       1.0\n",
      "grandma     10075   24     0     0   0    0     24       1.0\n",
      "------incels-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "housewife    4037   13     0     0   0    0     13       1.0\n",
      "daughters    6054   18     0    51   1    0     18       1.0\n",
      "bride        6054   21     0     2   4    0     21       1.0\n",
      "mommy        6054   34     0     0   0    0     34       1.0\n",
      "grandmother  6054   60     0     0   1    0     60       1.0\n",
      "------FDS-------\n",
      "       year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                  \n",
      "stacy  2019   16     0     0   0    0     16       1.0\n",
      "------PUA-------\n",
      "             year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                         \n",
      "community   22154   12     0    81  811    0     12       1.0\n",
      "hb1         14094   13     0     1    1    0     13       1.0\n",
      "hot blonde  16113   15     0     0    1    0     15       1.0\n",
      "wingwoman   16113   15     0     0    9    0     15       1.0\n",
      "milf        18135   16     0     4    6    1     16       1.0\n",
      "------CONTROL-------\n",
      "                 year  fem  masc  they    it  you  total  fem_frac\n",
      "word                                                              \n",
      "exgf            12085   11     0     0     3    0     11       1.0\n",
      "home mom        12095   12     0     2     0    0     12       1.0\n",
      "married couple  18135   12     0    48     1    0     12       1.0\n",
      "supermodel      14108   12     0     0     5    0     12       1.0\n",
      "community       28175   15     0   633  2782    1     15       1.0\n"
     ]
    }
   ],
   "source": [
    "for cat in cats: \n",
    "    print('------' + cat + '-------')\n",
    "    print(show_top_fem(cat, unmarked_all_df).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ff629",
   "metadata": {},
   "source": [
    "### Popular masc words in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "97ac1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_masc(cat, this_df): \n",
    "    cat_df = this_df[this_df.community == cat]\n",
    "    cat_totals = cat_df.groupby('word').sum()\n",
    "    cat_totals['total'] = cat_totals['fem'] + cat_totals['masc'] \n",
    "    # filter to only those that appear at least 10 times as she or he\n",
    "    cat_totals = cat_totals[cat_totals['total'] > 10] \n",
    "    cat_totals['masc_frac'] = cat_totals['masc'] / (cat_totals['fem'] + cat_totals['masc'])\n",
    "    cat_masc = cat_totals[cat_totals['masc_frac'] == 1]\n",
    "    return cat_masc.sort_values(by=['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "9b7ebb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Femcels-------\n",
      "Empty DataFrame\n",
      "Columns: [year, fem, masc, they, it, you, total, masc_frac]\n",
      "Index: []\n",
      "------the_attraction-------\n",
      "             year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                         \n",
      "cabbie      10040    0    11     0   1    0     11        1.0\n",
      "hitman      10047    0    11     0   0    1     11        1.0\n",
      "one kid     16079    0    13     0   0    0     13        1.0\n",
      "best buddy  18095    0    14     0   0    0     14        1.0\n",
      "captain     16078    0    15     0   0    0     15        1.0\n",
      "------TRP-------\n",
      "                    year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                                \n",
      "beginner           14112    0    11     2  13    0     11        1.0\n",
      "physicist          12099    0    11     0   1    0     11        1.0\n",
      "mailman            12093    0    11     0   2    0     11        1.0\n",
      "college professor  12099    0    11     0   0    0     11        1.0\n",
      "turk               10081    0    12     0   0    0     12        1.0\n",
      "------pua_forum-------\n",
      "          year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                      \n",
      "anybody  24138    0    11    98   1    2     11        1.0\n",
      "priest   12063    0    11     0   0    0     11        1.0\n",
      "devil    18106    0    12     2   4    0     12        1.0\n",
      "farmer   10050    0    13     1   2    1     13        1.0\n",
      "idiot    18109    0    13     0  25    0     13        1.0\n",
      "------MRA-------\n",
      "                  year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                              \n",
      "white kid        12089    0    11     0   0    0     11        1.0\n",
      "chef             14112    0    11     0   2    0     11        1.0\n",
      "public defender  12089    0    11     0   9    0     11        1.0\n",
      "foreman          12083    0    11     0   3    0     11        1.0\n",
      "grandad          18131    0    11     0   1    0     11        1.0\n",
      "------red_pill_talk-------\n",
      "            year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                        \n",
      "one kid     4029    0    11     0   0    0     11        1.0\n",
      "sluthater   4029    0    12     2   1    0     12        1.0\n",
      "coach       8063    0    13     0   0    0     13        1.0\n",
      "politician  8066    0    13     0   0    0     13        1.0\n",
      "prophet     8064    0    13     0   1    0     13        1.0\n",
      "------Incels-------\n",
      "                 year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                             \n",
      "cheater          6053    0    11     1   0    0     11        1.0\n",
      "goober           6054    0    11     0   0    0     11        1.0\n",
      "granddad         8064    0    11     0   0    0     11        1.0\n",
      "serial killers  16124    0    12    72   6    0     12        1.0\n",
      "mass murderer    8067    0    12     2   1    0     12        1.0\n",
      "------MGTOW-------\n",
      "               year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                           \n",
      "younger self   6051    0    11     0   0    1     11        1.0\n",
      "lone wolf      8070    0    11     0   3    0     11        1.0\n",
      "programmer     8070    0    13     2   3    0     13        1.0\n",
      "vp             6054    0    13     0   1    0     13        1.0\n",
      "chump         10085    0    14     3   4    0     14        1.0\n",
      "------rooshv-------\n",
      "            year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                        \n",
      "lone wolf  14104    0    11     1   6    0     11        1.0\n",
      "sensei     10080    0    11     1   1    0     11        1.0\n",
      "roomate    16112    0    11     0   5    0     11        1.0\n",
      "preacher   12099    0    11     1   0    0     11        1.0\n",
      "pal        14103    0    11     0   0    0     11        1.0\n",
      "------mgtow-------\n",
      "             year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                         \n",
      "salesman     8066    0    12     0   1    0     12        1.0\n",
      "coward       8066    0    13     0   1    0     13        1.0\n",
      "billionaire  8066    0    16     1   7    0     16        1.0\n",
      "chief        8066    0    16     0   1    0     16        1.0\n",
      "everyman     6051    0    16     1   0    0     16        1.0\n",
      "------avfm-------\n",
      "                  year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                              \n",
      "devil            12087    0    12     0  12    0     12        1.0\n",
      "player           10070    0    15     1   3    0     15        1.0\n",
      "amazing atheist   4027    0    16     0   2    0     16        1.0\n",
      "owner            14105    0    20     1   4    0     20        1.0\n",
      "priest           10075    0    21     1   0    0     21        1.0\n",
      "------incels-------\n",
      "                  year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                              \n",
      "villain           4037    0    11     3   2    0     11        1.0\n",
      "room mate         6054    0    12     0   0    0     12        1.0\n",
      "chad thundercock  6054    0    12     0   3    0     12        1.0\n",
      "enemy             6054    0    12    53  23    0     12        1.0\n",
      "perpetrator       4037    0    12     0   8    0     12        1.0\n",
      "------FDS-------\n",
      "                 year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                             \n",
      "current partner  2019    0    11     0   1    0     11        1.0\n",
      "------PUA-------\n",
      "               year  fem  masc  they  it  you  total  masc_frac\n",
      "word                                                           \n",
      "best buddy     8055    0    11     0   0    0     11        1.0\n",
      "billionaire   14102    0    11     3   4    0     11        1.0\n",
      "good wingman  14095    0    12     0   2    0     12        1.0\n",
      "millionaire   16118    0    13     0   0    0     13        1.0\n",
      "one kid       12092    0    13     0   0    0     13        1.0\n",
      "------CONTROL-------\n",
      "Empty DataFrame\n",
      "Columns: [year, fem, masc, they, it, you, total, masc_frac]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for cat in cats: \n",
    "    print('------' + cat + '-------')\n",
    "    print(show_top_masc(cat, unmarked_df).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2284fa",
   "metadata": {},
   "source": [
    "### Popular neut words in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "7d39375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_neut(cat, this_df): \n",
    "    cat_df = this_df[this_df.community == cat]\n",
    "    cat_totals = cat_df.groupby('word').sum()\n",
    "    cat_totals['total'] = cat_totals['fem'] + cat_totals['masc'] \n",
    "    # filter to only those that appear at least 10 times as she or he\n",
    "    cat_totals = cat_totals[cat_totals['total'] > 10] \n",
    "    cat_totals['fem_frac'] = cat_totals['fem'] / (cat_totals['fem'] + cat_totals['masc'])\n",
    "    cat_neut = cat_totals[cat_totals.fem_frac.between(0.48, 0.52)]\n",
    "    return cat_neut.sort_values(by=['total'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3a56c089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Femcels-------\n",
      "            year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                       \n",
      "partner     4037   58    58    33  31    0    116  0.500000\n",
      "child       4037   30    32     3  16    0     62  0.483871\n",
      "teacher     4037   26    25     0   0    0     51  0.509804\n",
      "looksmatch  4037   12    13     1  12    0     25  0.480000\n",
      "classmate   4037    6     6     0   0    0     12  0.500000\n",
      "------the_attraction-------\n",
      "              year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                         \n",
      "cousin       28161  247   239     2   5    4    486  0.508230\n",
      "friend here  16068   28    28     1   0    0     56  0.500000\n",
      "colleague    22122   25    26     0   0    0     51  0.490196\n",
      "caveman      18090   11    11     0   9    0     22  0.500000\n",
      "counselor    18090    9     9     0   0    0     18  0.500000\n",
      "------TRP-------\n",
      "           year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                       \n",
      "teacher   16124  364   383     2    1    5    747  0.487282\n",
      "asshole   14112  213   213    23  621    3    426  0.500000\n",
      "victim    14112  209   199     9  206    1    408  0.512255\n",
      "attorney  14112  118   115    15   10    0    233  0.506438\n",
      "neighbor  14112   76    80     2   19    0    156  0.487179\n",
      "------pua_forum-------\n",
      "               year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                          \n",
      "other friend  24138   56    58    11   0    3    114  0.491228\n",
      "colleague     20115   31    29     0   0    1     60  0.516667\n",
      "friend here   24138   26    26     0   0    0     52  0.500000\n",
      "same friend   16093   18    17     0   0    0     35  0.514286\n",
      "photographer  22132   13    13     5   3    0     26  0.500000\n",
      "------MRA-------\n",
      "            year   fem  masc  they    it  you  total  fem_frac\n",
      "word                                                          \n",
      "victim     24162  1376  1473   140  1177    2   2849  0.482976\n",
      "author     24162  1340  1434     0   305    1   2774  0.483057\n",
      "professor  24162   313   323     0    24    2    636  0.492138\n",
      "cop        22154   119   118     0   229    0    237  0.502110\n",
      "stranger   22154    52    52    21    50    0    104  0.500000\n",
      "------red_pill_talk-------\n",
      "          year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                     \n",
      "dentist   8062   14    15     0   1    0     29  0.482759\n",
      "roommate  6045    9     9     1   0    0     18  0.500000\n",
      "boyo      6045    6     6     1  10    0     12  0.500000\n",
      "hubby     2015    6     6     0   0    0     12  0.500000\n",
      "------Incels-------\n",
      "             year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                         \n",
      "teacher     18135  261   251     0    0    1    512  0.509766\n",
      "victim      18135   86    87    13  104    0    173  0.497110\n",
      "classmate   18135   59    60     0    0    0    119  0.495798\n",
      "stranger    18135   54    58    28   78    4    112  0.482143\n",
      "supervisor  16121   37    35     5    2    1     72  0.513889\n",
      "------MGTOW-------\n",
      "               year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                           \n",
      "baby          12099  635   618     3  587    8   1253  0.506784\n",
      "bro           12099  370   352     3   36  201    722  0.512465\n",
      "asshole       12099   81    84    12  180    1    165  0.490909\n",
      "supervisor     8070   47    44     1    9    0     91  0.516484\n",
      "acquaintance  10085   25    27     2    0    0     52  0.480769\n",
      "------rooshv-------\n",
      "             year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                        \n",
      "teacher     22154  211   208     0   0    2    419  0.503580\n",
      "hooker      20145   15    16    25  20    0     31  0.483871\n",
      "translator  16114    6     6     0   4    1     12  0.500000\n",
      "------mgtow-------\n",
      "            year  fem  masc  they   it  you  total  fem_frac\n",
      "word                                                        \n",
      "baby       12099  154   150     1  141    5    304  0.506579\n",
      "victim     12099   66    66     2   53    0    132  0.500000\n",
      "roommate   10080   29    30     0    0    0     59  0.491525\n",
      "bartender  12099   26    27     2    1    1     53  0.490566\n",
      "alcoholic  12099    7     7     0    6    0     14  0.500000\n",
      "------avfm-------\n",
      "            year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                       \n",
      "victim     14105   54    50     4  50    0    104  0.519231\n",
      "adult      14105   15    14     3   3    0     29  0.517241\n",
      "bartender   8056    6     6     0   0    1     12  0.500000\n",
      "blogger    10075    6     6     0   0    0     12  0.500000\n",
      "------incels-------\n",
      "              year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                         \n",
      "psychologist  6054   16    16     0   1    0     32       0.5\n",
      "lover         6054   12    12     0   0    0     24       0.5\n",
      "cop           6054   10    10     0  20    0     20       0.5\n",
      "teenager      6054   10    10     0   7    0     20       0.5\n",
      "stranger      4037    9     9     2   3    0     18       0.5\n",
      "------FDS-------\n",
      "         year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                    \n",
      "asshole  2019    7     7     1   8    0     14       0.5\n",
      "------PUA-------\n",
      "               year  fem  masc  they  it  you  total  fem_frac\n",
      "word                                                          \n",
      "drunk friend  16111   18    17     1   0    0     35  0.514286\n",
      "moron         12089    6     6     0   1    0     12  0.500000\n",
      "------CONTROL-------\n",
      "Empty DataFrame\n",
      "Columns: [year, fem, masc, they, it, you, total, fem_frac]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for cat in cats: \n",
    "    print('------' + cat + '-------')\n",
    "    print(show_top_neut(cat, unmarked_df).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6570d56",
   "metadata": {},
   "source": [
    "### Words with \"it\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8185ab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>community</th>\n",
       "      <th>word</th>\n",
       "      <th>fem</th>\n",
       "      <th>masc</th>\n",
       "      <th>they</th>\n",
       "      <th>it</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27024</td>\n",
       "      <td>2018</td>\n",
       "      <td>MGTOW</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>354</td>\n",
       "      <td>657</td>\n",
       "      <td>431</td>\n",
       "      <td>3230</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45598</td>\n",
       "      <td>2019</td>\n",
       "      <td>MGTOW</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>309</td>\n",
       "      <td>443</td>\n",
       "      <td>365</td>\n",
       "      <td>2513</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33362</td>\n",
       "      <td>2017</td>\n",
       "      <td>MGTOW</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>236</td>\n",
       "      <td>484</td>\n",
       "      <td>327</td>\n",
       "      <td>2164</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26988</td>\n",
       "      <td>2018</td>\n",
       "      <td>Incels</td>\n",
       "      <td>incels</td>\n",
       "      <td>140</td>\n",
       "      <td>179</td>\n",
       "      <td>3019</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15892</td>\n",
       "      <td>2016</td>\n",
       "      <td>MGTOW</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>82</td>\n",
       "      <td>199</td>\n",
       "      <td>170</td>\n",
       "      <td>989</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27124</td>\n",
       "      <td>2018</td>\n",
       "      <td>Incels</td>\n",
       "      <td>incel</td>\n",
       "      <td>170</td>\n",
       "      <td>1167</td>\n",
       "      <td>153</td>\n",
       "      <td>947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>2013</td>\n",
       "      <td>MRA</td>\n",
       "      <td>mrm</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>54</td>\n",
       "      <td>933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52965</td>\n",
       "      <td>2015</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>93</td>\n",
       "      <td>189</td>\n",
       "      <td>123</td>\n",
       "      <td>887</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49822</td>\n",
       "      <td>2017</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>89</td>\n",
       "      <td>198</td>\n",
       "      <td>127</td>\n",
       "      <td>878</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50430</td>\n",
       "      <td>2016</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>mgtow</td>\n",
       "      <td>58</td>\n",
       "      <td>151</td>\n",
       "      <td>93</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12466</td>\n",
       "      <td>2014</td>\n",
       "      <td>MRA</td>\n",
       "      <td>mrm</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "      <td>739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60093</td>\n",
       "      <td>2012</td>\n",
       "      <td>MRA</td>\n",
       "      <td>mrm</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4103</td>\n",
       "      <td>2015</td>\n",
       "      <td>TRP</td>\n",
       "      <td>alpha</td>\n",
       "      <td>171</td>\n",
       "      <td>575</td>\n",
       "      <td>86</td>\n",
       "      <td>517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>2013</td>\n",
       "      <td>MRA</td>\n",
       "      <td>child</td>\n",
       "      <td>445</td>\n",
       "      <td>952</td>\n",
       "      <td>6</td>\n",
       "      <td>489</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16057</td>\n",
       "      <td>2018</td>\n",
       "      <td>incels</td>\n",
       "      <td>incel</td>\n",
       "      <td>89</td>\n",
       "      <td>666</td>\n",
       "      <td>53</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year community    word  fem  masc  they    it  you\n",
       "27024  2018     MGTOW   mgtow  354   657   431  3230    6\n",
       "45598  2019     MGTOW   mgtow  309   443   365  2513    2\n",
       "33362  2017     MGTOW   mgtow  236   484   327  2164    2\n",
       "26988  2018    Incels  incels  140   179  3019  1097    4\n",
       "15892  2016     MGTOW   mgtow   82   199   170   989    3\n",
       "27124  2018    Incels   incel  170  1167   153   947    1\n",
       "1722   2013       MRA     mrm   15    22    54   933    0\n",
       "52965  2015     mgtow   mgtow   93   189   123   887    4\n",
       "49822  2017     mgtow   mgtow   89   198   127   878    5\n",
       "50430  2016     mgtow   mgtow   58   151    93   767    1\n",
       "12466  2014       MRA     mrm    4    22    52   739    0\n",
       "60093  2012       MRA     mrm    7    12    31   605    0\n",
       "4103   2015       TRP   alpha  171   575    86   517    1\n",
       "1859   2013       MRA   child  445   952     6   489    3\n",
       "16057  2018    incels   incel   89   666    53   477    0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['it'], ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fb964",
   "metadata": {},
   "source": [
    "### Gender over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cff22ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the words whose % fem ranges over time are the largest,\n",
    "# maybe top three words with biggest % range in each community, e.g. “cat\t10% - 50%”, only calculate fraction \n",
    "# if there are more than 10 occurrences in each community and month. \n",
    "\n",
    "def show_top_fem_range(cat):\n",
    "    # filtering for the argument category\n",
    "    cat_df = df[df.community == cat]\n",
    "    totals = cat_df.groupby(['year', 'word'], as_index = False).sum()\n",
    "    totals['total'] = totals['fem'] + totals['masc']\n",
    "    totals = totals[totals['total'] > 10]\n",
    "    totals['fem_frac'] = totals['fem'] / (totals['fem'] + totals['masc'])\n",
    "    \n",
    "    # filter for words that show up in more than 1 of the months/time periods \n",
    "    # (initially picked 85 to be more than half of all the months but idk if needed)\n",
    "    is_multi = totals[\"word\"].value_counts() > 1\n",
    "    filtered = totals[totals[\"word\"].isin(is_multi[is_multi].index)]\n",
    "    \n",
    "    # get the max and min fem_frac for each word\n",
    "    word_keys = filtered['word'].unique().tolist()\n",
    "    max_fems = []\n",
    "    min_fems = []\n",
    "    max_months = []\n",
    "    min_months = []\n",
    "    for word in word_keys: \n",
    "        df_subset = filtered[filtered['word'] == word]\n",
    "        max_fem = df_subset['fem_frac'].max()\n",
    "        min_fem = df_subset['fem_frac'].min()\n",
    "        max_month = df_subset[df_subset['fem_frac'] == max_fem]['year'].max()\n",
    "        min_month = df_subset[df_subset['fem_frac'] == min_fem]['year'].min()\n",
    "        \n",
    "        max_fems.append(max_fem)\n",
    "        min_fems.append(min_fem)\n",
    "        max_months.append(max_month)\n",
    "        min_months.append(min_month)\n",
    "    \n",
    "    \n",
    "    d = {'word': [], 'min month': [], 'min': [], 'max month': [], 'max':[], 'diff': []}\n",
    "    for i in range(len(word_keys)):\n",
    "        d['word'].append(word_keys[i])\n",
    "        d['min month'].append(min_months[i])\n",
    "        d['min'].append(min_fems[i])\n",
    "        d['max month'].append(max_months[i])\n",
    "        d['max'].append(max_fems[i])\n",
    "        d['diff'].append(max_fems[i] - min_fems[i])\n",
    "    \n",
    "    diffs = pd.DataFrame(data=d)\n",
    "    return diffs.sort_values(by = ['diff'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e6e713e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Femcels-------\n",
      "       word  min month       min  max month       max      diff\n",
      "11   doctor       2018  0.153846       2019  0.392857  0.239011\n",
      "35  partner       2018  0.320000       2019  0.549451  0.229451\n",
      "15   female       2018  0.705882       2019  0.894737  0.188854\n",
      "17  femcels       2018  0.312500       2019  0.461538  0.149038\n",
      "37      sis       2019  0.606061       2018  0.733333  0.127273\n",
      "------the_attraction-------\n",
      "           word  min month       min  max month       max      diff\n",
      "136      waiter       2007  0.071429       2008  0.562500  0.491071\n",
      "22      asshole       2006  0.421053       2010  0.909091  0.488038\n",
      "147  one friend       2008  0.166667       2009  0.615385  0.448718\n",
      "128    stranger       2006  0.285714       2009  0.727273  0.441558\n",
      "64          gal       2006  0.500000       2007  0.923077  0.423077\n",
      "------TRP-------\n",
      "            word  min month       min  max month       max      diff\n",
      "398       spouse       2019  0.200000       2016  0.816901  0.616901\n",
      "442   beta bitch       2016  0.461538       2018  1.000000  0.538462\n",
      "301  male friend       2018  0.117647       2017  0.642857  0.525210\n",
      "304          men       2018  0.066667       2016  0.538462  0.471795\n",
      "210         cops       2015  0.222222       2014  0.692308  0.470085\n",
      "------pua_forum-------\n",
      "             word  min month       min  max month       max      diff\n",
      "16    best friend       2015  0.300000       2017  0.727273  0.427273\n",
      "14      bartender       2010  0.576923       2017  1.000000  0.423077\n",
      "108  someone else       2009  0.133333       2010  0.500000  0.366667\n",
      "82          child       2010  0.117647       2013  0.450000  0.332353\n",
      "11     alpha male       2013  0.142857       2014  0.473684  0.330827\n",
      "------MRA-------\n",
      "          word  min month       min  max month       max      diff\n",
      "213       prof       2018  0.083333       2012  0.769231  0.685897\n",
      "103       mras       2016  0.272727       2011  0.818182  0.545455\n",
      "140     artist       2017  0.090909       2018  0.625000  0.534091\n",
      "238  bartender       2017  0.250000       2019  0.769231  0.519231\n",
      "156       cops       2016  0.347826       2013  0.857143  0.509317\n",
      "------red_pill_talk-------\n",
      "           word  min month       min  max month       max      diff\n",
      "62           jb       2014  0.431818       2016  0.794872  0.363054\n",
      "11  best friend       2014  0.000000       2015  0.304348  0.304348\n",
      "5         aspie       2015  0.000000       2014  0.263158  0.263158\n",
      "18         boss       2015  0.027027       2016  0.250000  0.222973\n",
      "91          pua       2018  0.000000       2016  0.209677  0.209677\n",
      "------Incels-------\n",
      "           word  min month       min  max month       max      diff\n",
      "13   girlfriend       2019  0.095386       2013  0.964103  0.868717\n",
      "146  supervisor       2018  0.214286       2016  0.764706  0.550420\n",
      "131    neighbor       2019  0.277778       2016  0.818182  0.540404\n",
      "300  sex worker       2018  0.466667       2019  1.000000  0.533333\n",
      "119      fiance       2018  0.384615       2019  0.913043  0.528428\n",
      "------MGTOW-------\n",
      "             word  min month       min  max month       max      diff\n",
      "275       patient       2017  0.083333       2018  0.583333  0.500000\n",
      "311    stepfather       2019  0.125000       2018  0.529412  0.404412\n",
      "197  ex boyfriend       2017  0.333333       2018  0.714286  0.380952\n",
      "174     beta male       2017  0.076923       2019  0.454545  0.377622\n",
      "189          cops       2019  0.222222       2018  0.583333  0.361111\n",
      "------avfm-------\n",
      "         word  min month       min  max month       max      diff\n",
      "27     doctor       2016  0.062500       2014  0.409091  0.346591\n",
      "46  professor       2013  0.230769       2014  0.538462  0.307692\n",
      "22     author       2015  0.258065       2014  0.549020  0.290955\n",
      "38       male       2017  0.117647       2015  0.375000  0.257353\n",
      "7      friend       2015  0.351351       2012  0.600000  0.248649\n",
      "------rooshv-------\n",
      "           word  min month       min  max month       max      diff\n",
      "95      partner       2012  0.142857       2018  0.692308  0.549451\n",
      "15      someone       2019  0.037500       2010  0.461538  0.424038\n",
      "44         male       2012  0.138889       2017  0.562500  0.423611\n",
      "199  journalist       2017  0.083333       2015  0.500000  0.416667\n",
      "59      teacher       2015  0.313253       2019  0.727273  0.414020\n",
      "------mgtow-------\n",
      "         word  min month       min  max month       max      diff\n",
      "48      daddy       2017  0.150000       2015  0.611111  0.461111\n",
      "107    prince       2016  0.000000       2018  0.357143  0.357143\n",
      "47   coworker       2016  0.386364       2017  0.742857  0.356494\n",
      "75    mangina       2015  0.085714       2018  0.416667  0.330952\n",
      "113    rapist       2015  0.181818       2016  0.500000  0.318182\n",
      "------incels-------\n",
      "             word  min month       min  max month       max      diff\n",
      "19        dentist       2018  0.200000       2017  0.545455  0.345455\n",
      "92             ex       2019  0.111111       2018  0.447368  0.336257\n",
      "128  psychologist       2018  0.375000       2019  0.666667  0.291667\n",
      "29            fho       2017  0.739130       2018  1.000000  0.260870\n",
      "16           cunt       2018  0.669421       2017  0.928571  0.259150\n",
      "------FDS-------\n",
      "Empty DataFrame\n",
      "Columns: [word, min month, min, max month, max, diff]\n",
      "Index: []\n",
      "------PUA-------\n",
      "              word  min month       min  max month       max      diff\n",
      "87     good friend       2015  0.181818       2019  0.666667  0.484848\n",
      "103     one friend       2012  0.192308       2011  0.666667  0.474359\n",
      "150   other person       2013  0.222222       2017  0.666667  0.444444\n",
      "35   mutual friend       2019  0.500000       2010  0.937500  0.437500\n",
      "125        teacher       2014  0.318182       2017  0.750000  0.431818\n"
     ]
    }
   ],
   "source": [
    "# words in each month in Incels that appear more than 10 times in that month\n",
    "for cat in cats: \n",
    "    print('------' + cat + '-------')\n",
    "    print(show_top_fem_range(cat).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7ef80",
   "metadata": {},
   "source": [
    "### Gender differences\n",
    "\n",
    "Lucy hasn't edited this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0bd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e70b693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fem</th>\n",
       "      <th>masc</th>\n",
       "      <th>neut</th>\n",
       "      <th>total</th>\n",
       "      <th>fem_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>girl</td>\n",
       "      <td>308205</td>\n",
       "      <td>2567</td>\n",
       "      <td>226</td>\n",
       "      <td>310772</td>\n",
       "      <td>0.991740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>woman</td>\n",
       "      <td>248514</td>\n",
       "      <td>1024</td>\n",
       "      <td>838</td>\n",
       "      <td>249538</td>\n",
       "      <td>0.995896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>wife</td>\n",
       "      <td>74069</td>\n",
       "      <td>576</td>\n",
       "      <td>55</td>\n",
       "      <td>74645</td>\n",
       "      <td>0.992283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mother</td>\n",
       "      <td>31271</td>\n",
       "      <td>275</td>\n",
       "      <td>139</td>\n",
       "      <td>31546</td>\n",
       "      <td>0.991283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mom</td>\n",
       "      <td>23830</td>\n",
       "      <td>893</td>\n",
       "      <td>186</td>\n",
       "      <td>24723</td>\n",
       "      <td>0.963880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physicist</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tall man</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soccer player</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>great leader</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>weird guy</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1918 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fem  masc  neut   total  fem_frac\n",
       "word                                               \n",
       "girl           308205  2567   226  310772  0.991740\n",
       "woman          248514  1024   838  249538  0.995896\n",
       "wife            74069   576    55   74645  0.992283\n",
       "mother          31271   275   139   31546  0.991283\n",
       "mom             23830   893   186   24723  0.963880\n",
       "...               ...   ...   ...     ...       ...\n",
       "physicist           0    29     1      29  0.000000\n",
       "tall man            0    59     1      59  0.000000\n",
       "soccer player       0    11     0      11  0.000000\n",
       "great leader        0    12     0      12  0.000000\n",
       "weird guy           0    28     0      28  0.000000\n",
       "\n",
       "[1918 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(df.word.unique()\n",
    "df = df.groupby('word').sum()\n",
    "df['total'] = df['fem'] + df['masc']\n",
    "df = df[df['total'] > 10] \n",
    "df['fem_frac'] = df['fem'] / (df['fem'] + df['masc'])\n",
    "df = df.sort_values(by=['fem'], ascending = False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4e16d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fem</th>\n",
       "      <th>masc</th>\n",
       "      <th>neut</th>\n",
       "      <th>total</th>\n",
       "      <th>fem_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>mom</td>\n",
       "      <td>22693</td>\n",
       "      <td>764</td>\n",
       "      <td>200</td>\n",
       "      <td>23457</td>\n",
       "      <td>0.967430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>wife</td>\n",
       "      <td>22389</td>\n",
       "      <td>165</td>\n",
       "      <td>18</td>\n",
       "      <td>22554</td>\n",
       "      <td>0.992684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>girl</td>\n",
       "      <td>22147</td>\n",
       "      <td>269</td>\n",
       "      <td>29</td>\n",
       "      <td>22416</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>woman</td>\n",
       "      <td>17238</td>\n",
       "      <td>131</td>\n",
       "      <td>56</td>\n",
       "      <td>17369</td>\n",
       "      <td>0.992458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mother</td>\n",
       "      <td>13385</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>13492</td>\n",
       "      <td>0.992069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>composer</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>common man</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>colonel</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>college kid</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lineman</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               fem  masc  neut  total  fem_frac\n",
       "word                                           \n",
       "mom          22693   764   200  23457  0.967430\n",
       "wife         22389   165    18  22554  0.992684\n",
       "girl         22147   269    29  22416  0.988000\n",
       "woman        17238   131    56  17369  0.992458\n",
       "mother       13385   107    53  13492  0.992069\n",
       "...            ...   ...   ...    ...       ...\n",
       "composer         0    29     7     29  0.000000\n",
       "common man       0    34     1     34  0.000000\n",
       "colonel          0    25     0     25  0.000000\n",
       "college kid      0    13     1     13  0.000000\n",
       "lineman          0    12     2     12  0.000000\n",
       "\n",
       "[1140 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df = control_df.groupby('word').sum()\n",
    "control_df['total'] = control_df['fem'] + control_df['masc']\n",
    "control_df = control_df[control_df['total'] > 10] \n",
    "control_df['fem_frac'] = control_df['fem'] / (control_df['fem'] + control_df['masc'])\n",
    "control_df = control_df.sort_values(by=['fem'], ascending = False)\n",
    "control_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f35b531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fem_x</th>\n",
       "      <th>masc_x</th>\n",
       "      <th>neut_x</th>\n",
       "      <th>total_x</th>\n",
       "      <th>fem_frac_x</th>\n",
       "      <th>fem_y</th>\n",
       "      <th>masc_y</th>\n",
       "      <th>neut_y</th>\n",
       "      <th>total_y</th>\n",
       "      <th>fem_frac_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>girl</td>\n",
       "      <td>308205</td>\n",
       "      <td>2567</td>\n",
       "      <td>226</td>\n",
       "      <td>310772</td>\n",
       "      <td>0.991740</td>\n",
       "      <td>22147</td>\n",
       "      <td>269</td>\n",
       "      <td>29</td>\n",
       "      <td>22416</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>woman</td>\n",
       "      <td>248514</td>\n",
       "      <td>1024</td>\n",
       "      <td>838</td>\n",
       "      <td>249538</td>\n",
       "      <td>0.995896</td>\n",
       "      <td>17238</td>\n",
       "      <td>131</td>\n",
       "      <td>56</td>\n",
       "      <td>17369</td>\n",
       "      <td>0.992458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>wife</td>\n",
       "      <td>74069</td>\n",
       "      <td>576</td>\n",
       "      <td>55</td>\n",
       "      <td>74645</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>22389</td>\n",
       "      <td>165</td>\n",
       "      <td>18</td>\n",
       "      <td>22554</td>\n",
       "      <td>0.992684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mother</td>\n",
       "      <td>31271</td>\n",
       "      <td>275</td>\n",
       "      <td>139</td>\n",
       "      <td>31546</td>\n",
       "      <td>0.991283</td>\n",
       "      <td>13385</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>13492</td>\n",
       "      <td>0.992069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mom</td>\n",
       "      <td>23830</td>\n",
       "      <td>893</td>\n",
       "      <td>186</td>\n",
       "      <td>24723</td>\n",
       "      <td>0.963880</td>\n",
       "      <td>22693</td>\n",
       "      <td>764</td>\n",
       "      <td>200</td>\n",
       "      <td>23457</td>\n",
       "      <td>0.967430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>layman</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>first man</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physicist</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tall man</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>great leader</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1038 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               fem_x  masc_x  neut_x  total_x  fem_frac_x  fem_y  masc_y  \\\n",
       "word                                                                       \n",
       "girl          308205    2567     226   310772    0.991740  22147     269   \n",
       "woman         248514    1024     838   249538    0.995896  17238     131   \n",
       "wife           74069     576      55    74645    0.992283  22389     165   \n",
       "mother         31271     275     139    31546    0.991283  13385     107   \n",
       "mom            23830     893     186    24723    0.963880  22693     764   \n",
       "...              ...     ...     ...      ...         ...    ...     ...   \n",
       "layman             0      11       0       11    0.000000      1      11   \n",
       "first man          0      69       0       69    0.000000      0      31   \n",
       "physicist          0      29       1       29    0.000000      1      22   \n",
       "tall man           0      59       1       59    0.000000      0      17   \n",
       "great leader       0      12       0       12    0.000000      0      19   \n",
       "\n",
       "              neut_y  total_y  fem_frac_y  \n",
       "word                                       \n",
       "girl              29    22416    0.988000  \n",
       "woman             56    17369    0.992458  \n",
       "wife              18    22554    0.992684  \n",
       "mother            53    13492    0.992069  \n",
       "mom              200    23457    0.967430  \n",
       "...              ...      ...         ...  \n",
       "layman             0       12    0.083333  \n",
       "first man          0       31    0.000000  \n",
       "physicist          2       23    0.043478  \n",
       "tall man           0       17    0.000000  \n",
       "great leader       0       19    0.000000  \n",
       "\n",
       "[1038 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = df.merge(control_df, how='inner', left_index=True, right_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3702c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['difference'] = (merged_df['fem_frac_x'] - merged_df['fem_frac_y']).abs()\n",
    "merged_df = merged_df.sort_values(by=['difference'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f1d181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fem_x</th>\n",
       "      <th>masc_x</th>\n",
       "      <th>neut_x</th>\n",
       "      <th>total_x</th>\n",
       "      <th>fem_frac_x</th>\n",
       "      <th>fem_y</th>\n",
       "      <th>masc_y</th>\n",
       "      <th>neut_y</th>\n",
       "      <th>total_y</th>\n",
       "      <th>fem_frac_y</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>brat</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.641026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>expert</td>\n",
       "      <td>68</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>103</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.517337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>band</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>382</td>\n",
       "      <td>17</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1799</td>\n",
       "      <td>21</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.501401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cunt</td>\n",
       "      <td>789</td>\n",
       "      <td>205</td>\n",
       "      <td>60</td>\n",
       "      <td>994</td>\n",
       "      <td>0.793763</td>\n",
       "      <td>35</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.481263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sitter</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.452381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>trans woman</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>grown woman</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>own daughter</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>great woman</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>great leader</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1038 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              fem_x  masc_x  neut_x  total_x  fem_frac_x  fem_y  masc_y  \\\n",
       "word                                                                      \n",
       "brat             21       5       1       26    0.807692      2      10   \n",
       "expert           68      35      25      103    0.660194      8      48   \n",
       "band             15       2     382       17    0.882353      8      13   \n",
       "cunt            789     205      60      994    0.793763     35      77   \n",
       "sitter           11       3       1       14    0.785714      7      14   \n",
       "...             ...     ...     ...      ...         ...    ...     ...   \n",
       "trans woman     131       0       1      131    1.000000     73       0   \n",
       "grown woman     120       0       5      120    1.000000     32       0   \n",
       "own daughter    112       0       0      112    1.000000     48       0   \n",
       "great woman     103       0       0      103    1.000000     12       0   \n",
       "great leader      0      12       0       12    0.000000      0      19   \n",
       "\n",
       "              neut_y  total_y  fem_frac_y  difference  \n",
       "word                                                   \n",
       "brat               1       12    0.166667    0.641026  \n",
       "expert            31       56    0.142857    0.517337  \n",
       "band            1799       21    0.380952    0.501401  \n",
       "cunt              12      112    0.312500    0.481263  \n",
       "sitter             0       21    0.333333    0.452381  \n",
       "...              ...      ...         ...         ...  \n",
       "trans woman        4       73    1.000000    0.000000  \n",
       "grown woman        0       32    1.000000    0.000000  \n",
       "own daughter       0       48    1.000000    0.000000  \n",
       "great woman        0       12    1.000000    0.000000  \n",
       "great leader       0       19    0.000000    0.000000  \n",
       "\n",
       "[1038 rows x 11 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80240e8d",
   "metadata": {},
   "source": [
    "### Pronoun sparsity\n",
    "\n",
    "Lucy hasn't edited this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5f7d1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fem</th>\n",
       "      <th>masc</th>\n",
       "      <th>neut</th>\n",
       "      <th>total</th>\n",
       "      <th>neut_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>zombies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>monarchs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mobile users</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mockingbird</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>moderate feminists</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>walking wallet</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>total loser</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>changs</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chick magnet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tradesman</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fem  masc  neut  total  neut_frac\n",
       "word                                                 \n",
       "zombies               0     0    97     97        1.0\n",
       "monarchs              0     0    18     18        1.0\n",
       "mobile users          0     0     1      1        1.0\n",
       "mockingbird           0     0     1      1        1.0\n",
       "moderate feminists    0     0    86     86        1.0\n",
       "...                 ...   ...   ...    ...        ...\n",
       "walking wallet        2     0     2      4        0.5\n",
       "total loser           0     2     2      4        0.5\n",
       "changs                0     2     2      4        0.5\n",
       "chick magnet          1     0     1      2        0.5\n",
       "tradesman             0     8     8     16        0.5\n",
       "\n",
       "[3716 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pronoun_df.csv')\n",
    "# df = df.groupby('word').sum()\n",
    "# df.shape[0]\n",
    "\n",
    "# total vocab words that show up in reddit with masc/fem/neut pronouns is 6373\n",
    "\n",
    "df_totals = df.groupby('word').sum()\n",
    "# df_totals['total'] = df_totals['fem'] + df_totals['masc'] \n",
    "df_totals['total'] = df_totals['fem'] + df_totals['masc'] + df_totals['neut']\n",
    "\n",
    "df_totals['neut_frac'] = df_totals['neut'] / (df_totals['fem'] + df_totals['masc'] + df_totals['neut'])\n",
    "df_totals \n",
    "\n",
    "df_neut = df_totals[df_totals['neut_frac'] >= 0.5].sort_values(by = ['neut_frac'], ascending = False)\n",
    "# df_neut.head(20)\n",
    "df_neut\n",
    "\n",
    "# df_sparse = df_totals[df_totals['total'] <= 10]\n",
    "# df_sparse\n",
    "\n",
    "\n",
    "\n",
    "# 4300 words have less than 10 occurrences with masc or fem pronouns => ~70%\n",
    "# 1438 words have less than 10 occurrences with masc or fem or neut pronouns => ~23%\n",
    "\n",
    "# 3716 words are mostly \"they\" words => ~60%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec429c",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "This code compares booknlp coref vs. spacy coref on hand-labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4d5e88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_masc = set()\n",
    "gold_fem = set()\n",
    "with open(ROOT + 'logs/gender_gold_labels.csv', 'r') as infile: \n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader: \n",
    "        if row['gendered?'] == 'm':\n",
    "            gold_masc.add(row['word (singular)'].lower())\n",
    "        if row['gendered?'] == 'f': \n",
    "            gold_fem.add(row['word (singular)'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dbc95e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "david_labels = Counter()\n",
    "with open(ROOT + 'logs/temp_gender.txt', 'r') as infile: \n",
    "    reader = csv.DictReader(infile, delimiter='\\t')\n",
    "    for row in reader: \n",
    "        if row['proper'] != 'nom': continue\n",
    "        if (float(row['he/him/his']) + float(row['she/her'])) < 3: continue\n",
    "        david_labels[row['term']] = float(row['she/her']) / (float(row['he/him/his']) + float(row['she/her']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9d393736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(COREF_RESULTS + 'coref_reddit_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "90357864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('word').sum()\n",
    "df['fem_frac'] = df['fem'] / (df['fem'] + df['masc'])\n",
    "df = df[['fem_frac']].dropna()\n",
    "df = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "99ca6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_labels = df['fem_frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5ee87d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masc words\n",
      "SPACY: 0.20069466570475966 BOOKNLP: 0.31480068170482933\n"
     ]
    }
   ],
   "source": [
    "# average score for m words\n",
    "spacy_scores = []\n",
    "david_scores = []\n",
    "for w in gold_masc: \n",
    "    if w in spacy_labels and w in david_labels: \n",
    "        spacy_scores.append(spacy_labels[w])\n",
    "        david_scores.append(david_labels[w])\n",
    "print(\"masc words\")\n",
    "print(\"SPACY:\", np.mean(spacy_scores), \"BOOKNLP:\", np.mean(david_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ec65e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fem words\n",
      "SPACY: 0.8515987971507715 BOOKNLP: 0.7529589257733309\n"
     ]
    }
   ],
   "source": [
    "# average score for f words\n",
    "spacy_scores = []\n",
    "david_scores = []\n",
    "for w in gold_fem: \n",
    "    if w in spacy_labels and w in david_labels: \n",
    "        spacy_scores.append(spacy_labels[w])\n",
    "        david_scores.append(david_labels[w])\n",
    "print(\"fem words\")\n",
    "print(\"SPACY:\", np.mean(spacy_scores), \"BOOKNLP:\", np.mean(david_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578a080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
